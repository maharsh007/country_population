{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/80216583/Downloads/tutorials/python-projects/country-sort/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# See how many tokens are in the vocabulary\n",
    "tokenizer.vocab_size\n",
    "# 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'heart', 'genera', '##tive', 'ai']\n",
      "[1045, 2540, 11416, 6024, 9932]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(\"I heart Generative AI\")\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n",
    "# ['i', 'heart', 'genera', '##tive', 'ai']\n",
    "\n",
    "# Show the token ids assigned to each token\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))\n",
    "# [1045, 2540, 11416, 6024, 9932]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (88.68%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the input sequence\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "\n",
    "# Display sentiment result\n",
    "if predicted_class == 1:\n",
    "    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "# Sentiment: Positive (88.68%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.10-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.10-cp311-cp311-macosx_10_9_x86_64.whl (468 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-macosx_12_0_x86_64.whl (30.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.9/30.9 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_10_9_x86_64.whl (31 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-macosx_10_9_x86_64.whl (54 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl (29 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl (45 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-macosx_10_9_x86_64.whl (94 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.2 attrs-24.3.0 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "I have read all of the Love Come Softly books. Knowing full well that movies can not use all aspects of the book,but generally they at least have the main point of the book. I was highly disappointed in this movie. The only thing that they have in this movie that is in the book is that Missy's father comes to visit,(although in the book both parents come). That is all. The story line was so twisted and far fetch and yes, sad, from the book, that ..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the IMDB dataset, which contains movie reviews\n",
    "# and sentiment labels (positive or negative)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Fetch a revie from the training set\n",
    "review_number = 40\n",
    "sample_review = dataset[\"train\"][review_number]\n",
    "\n",
    "display(HTML(sample_review[\"text\"][:450] + \"...\"))\n",
    "# WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the\n",
    "# film.\n",
    "# \n",
    "# With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they\n",
    "# see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does.\n",
    "# Did they even think to check on the director's credentials...\n",
    "\n",
    "if sample_review[\"label\"] == 1:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")\n",
    "# Sentiment: Negative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
